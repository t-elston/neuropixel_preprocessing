{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated spike sorting, quality metrics, and data alignment. \n",
    "This is a pipeline which spike-sorts neuropixel data aquired with [SpikeGLX](https://billkarsh.github.io/SpikeGLX/) and task control via either [NIMH MonkeyLogic](https://monkeylogic.nimh.nih.gov/index.html) or [PsychToolbox](https://psychtoolbox.org/). \n",
    "\n",
    "It makes use of the [Spike Interface](https://spikeinterface.readthedocs.io/en/latest/) framework and assumes\n",
    "that [Kilosort 3](https://github.com/MouseLand/Kilosort) is installed on this machine.\n",
    "\n",
    "The program will also extract the sync edges associated with each data stream and then map all of your data into a common timeline (the timeline of your first neuropixels probe). \n",
    "\n",
    "The program also extracts 8 bit digital words used to mark specific [task events](https://monkeylogic.nimh.nih.gov/docs_RuntimeFunctions.html#eventmarker). These are also mapped into a master timeline. \n",
    "\n",
    "This version was created by [Thom Elston](https://www.thomelston.com/) in December 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spikeinterface.full as si\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import multiprocessing\n",
    "import neuropixel_preprocessing_module as npm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the only user defined parameters - everything else is automated!\n",
    "\n",
    "The things you need to change are `base_folder`, `brain_areas`, and `kilosort3_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting KILOSORT3_PATH environment variable for subprocess calls to: C:\\Users\\Thomas Elston\\Documents\\MATLAB\\Kilosort-3\n"
     ]
    }
   ],
   "source": [
    "# highest level folder for this recording\n",
    "base_folder = Path('D:/D20231231_Rec10_g0/')\n",
    "\n",
    "run_spike_sorter = False\n",
    "\n",
    "# subfolders for each probe\n",
    "probe_folders = [folder for folder in base_folder.glob('*') if folder.is_dir()]\n",
    "\n",
    "# which brain area is probe 0 and 1?\n",
    "brain_areas = ['CdN', 'OFC']\n",
    "\n",
    "# save a csv with brain_areas info\n",
    "pd.DataFrame(brain_areas, columns=['brain_area']).to_csv(base_folder / 'brain_areas.csv')\n",
    "\n",
    "# set the path to kilosort 3, which we'll use to spike sort\n",
    "kilosort3_path = 'C:/Users/Thomas Elston/Documents/MATLAB/Kilosort-3'\n",
    "si.Kilosort3Sorter.set_kilosort3_path(kilosort3_path)\n",
    "\n",
    "# get the default sorting parameters for ks3 and change a few for our purposes\n",
    "default_ks3_params = si.get_default_sorter_params('kilosort3')\n",
    "params_kilosort3 = dict(projection_threshold= [10, 4])\n",
    "\n",
    "# figure out how many cores are available on this machine \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# set parameters for parallelized operations\n",
    "job_kwargs = dict(n_jobs=num_cores-2, chunk_duration='1s', progress_bar=True)\n",
    "\n",
    "# should we delete the intermediate files generated by preprocessing + sorting?\n",
    "delete_intermediate = True\n",
    "\n",
    "# initialize a list to accumulate the names of the sync_files into\n",
    "# - these are what are used to map data into a common timeline\n",
    "sync_files = []\n",
    "data_stream_info = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec0 in CdN\n",
      "\n",
      "Loading D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec1 in OFC\n",
      "\n",
      "Finished all files. :)\n"
     ]
    }
   ],
   "source": [
    "# Start looping over each probe\n",
    "for i in range(len(probe_folders)):\n",
    "\n",
    "    i_probe = probe_folders[i]\n",
    "    i_brain_area = brain_areas[i]\n",
    "\n",
    "    print('Loading ' + str(i_probe) + ' in ' + str(i_brain_area) + '\\n')\n",
    "\n",
    "    # read and verify the data streams for this probe\n",
    "    stream_names, stream_ids = si.get_neo_streams('spikeglx', i_probe)\n",
    "\n",
    "    # collect the name of the sync_files for later\n",
    "    sync_files.append(npm.get_path_from_dir(i_probe, 'lf.bin'))\n",
    "    data_stream_info.append('imec'+ str(i))\n",
    "\n",
    "    # get the action-potential data stream\n",
    "    ap_stream = stream_names['.ap' in stream_names]\n",
    "    if run_spike_sorter:\n",
    "        # we do not load the sync channel, so the probe is automatically loaded\n",
    "        raw_rec = si.read_spikeglx(i_probe, stream_name=ap_stream, load_sync_channel=False)\n",
    "\n",
    "        print('Bandpassing the signal.')\n",
    "        # do a series of signal preprocessing steps:\n",
    "        # 1. bandpass the data\n",
    "        rec1 = si.highpass_filter(raw_rec, freq_min=300.)\n",
    "\n",
    "        # 2. apply a shift correction to account for multiplexing error\n",
    "        print('Correcting multiplexing temporal shift...')\n",
    "        rec2 = si.phase_shift(rec1)\n",
    "\n",
    "        rec = rec2\n",
    "\n",
    "        # now save the preprocessed data for use in kilosort 3\n",
    "        print('Saving preprocessed data... \\n')\n",
    "        rec = rec.save(folder=i_probe / 'preprocess', format='binary', **job_kwargs)\n",
    "\n",
    "        # run kilosort 3\n",
    "        print('Running kilosort 3... \\n')\n",
    "        out_name = i_probe / 'ks3_out'\n",
    "        sorting = si.run_sorter('kilosort3', rec, output_folder=out_name, verbose=True, **params_kilosort3)\n",
    "\n",
    "        # now extract waveforms and spike positions to compute quality metrics\n",
    "        print('Extracting waveforms for QC metrics...')\n",
    "        we = si.extract_waveforms(rec, sorting, folder= i_probe / 'waveforms_ks3',\n",
    "                            sparse=True, max_spikes_per_unit=1000, ms_before=1.5,ms_after=2.,\n",
    "                            **job_kwargs)\n",
    "        si.compute_spike_locations(we)\n",
    "\n",
    "        # compute quality metrics\n",
    "        print('Computing QC metrics...')\n",
    "        metrics = si.compute_quality_metrics(we, metric_names=['firing_rate', 'presence_ratio', 'snr',\n",
    "                                                        'isi_violation', 'drift','amplitude_median', 'amplitude_cutoff'])\n",
    "        \n",
    "        # save the quality metrics\n",
    "        metrics_save_name = i_probe / 'ks3_out' / 'sorter_output' / 'quality_metrics.csv'\n",
    "        metrics.to_csv(metrics_save_name)\n",
    "\n",
    "        # check if we should delete the intermediate files\n",
    "        if delete_intermediate:\n",
    "            print('Deleting intermediate files...')\n",
    "            files_to_delete = [i_probe / 'ks3_out' / 'sorter_output' / 'temp_wh.dat',\n",
    "                            i_probe / 'preprocess' / 'traces_cached_seg0.raw']\n",
    "            \n",
    "            # Delete each file\n",
    "            for file_path in files_to_delete:\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"Deleted: {file_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting {file_path}: {e}\")\n",
    "\n",
    "        print('Finished preprocessing and sorting in ' + i_brain_area + '\\n')\n",
    "\n",
    "print('Finished all files. :)')          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract the sync edges and map the spike times to a reference timeline\n",
    "\n",
    "**NOTE:** This assumes that analog channel 0 is used as the sync data stream for the nidaq stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sync edges...\n",
      "\n",
      "n_channels: 385, n_file_samples: 12614244\n",
      "n_channels: 385, n_file_samples: 12614371\n",
      "n_channels: 9, n_file_samples: 53450078\n",
      "\n",
      "Sync edge times saved in original directory.\n",
      "Extracting event codes from nidq stream...\n",
      "\n",
      "n_channels: 9, n_file_samples: 53450078\n",
      "\n",
      "Event codes saved in original directory.\n"
     ]
    }
   ],
   "source": [
    "# add the nidaq data to sync_files and now extract the edges associated with each data stream. \n",
    "sync_files.append(npm.get_path_from_dir(base_folder, 'nidq.bin'))\n",
    "data_stream_info.append('nidq')\n",
    "npm.extract_sync_edges(sync_files)\n",
    "\n",
    "# extract 8 bit event words to mark task events (from e.g. MonkeyLogic or Psych Toolbox)\n",
    "npm.extract_event_codes(npm.get_path_from_dir(base_folder, 'nidq.bin'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping spikes and task events to a common timeline.\n",
      "\n",
      "imec0 set as master timeline.\n",
      "aligning: D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec0\n",
      "aligning: D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec1\n",
      "aligning: D:\\D20231231_Rec10_g0\n",
      "\n",
      "Aligned spikes and event codes saved in their original directories.\n"
     ]
    }
   ],
   "source": [
    "# map your data into a common timeline\n",
    "npm.align_data_streams(sync_files, data_stream_info, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and mapping LFPs to a common timeline.\n",
      "\n",
      "imec0 set as master timeline.\n",
      "aligning: D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec0\n",
      "n_channels: 385, n_file_samples: 12614244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec0: 100%|██████████| 385/385 [03:17<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligning: D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec1\n",
      "n_channels: 385, n_file_samples: 12614371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\D20231231_Rec10_g0\\D20231231_Rec10_g0_imec1: 100%|██████████| 385/385 [03:19<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aligned and downsampled LFPs saved in their original directories.\n"
     ]
    }
   ],
   "source": [
    "# extract and align the LFP streams\n",
    "npm.align_LFP_streams(sync_files, data_stream_info, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
